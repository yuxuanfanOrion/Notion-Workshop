<!-- notion-id: 2bf2a5ae-4040-808a-b886-dfdb98749acf -->
| ***Video Foundation model*** |  |  |  |  |
| --- | --- | --- | --- | --- |
| Video Mamba Suite |  |  |  |  |
| Scaling RL to Long Videos (Nvilab) | [https://arxiv.org/pdf/2507.07966](https://arxiv.org/pdf/2507.07966) | NeurIPS |  |  |
| Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model | [https://arxiv.org/pdf/2503.21782](https://arxiv.org/pdf/2503.21782) |  |  |  |
| SmolVLM: Redefining small and efficient multimodal models | [https://arxiv.org/abs/2504.05299](https://arxiv.org/abs/2504.05299) |  |  |  |
| Kwai Keye-VL Technical Report | [https://arxiv.org/pdf/2507.01949](https://arxiv.org/pdf/2507.01949) | technical report |  |  |
| Seed1.5-VL Technical Report | [https://arxiv.org/pdf/2505.07062](https://arxiv.org/pdf/2505.07062) | technical report |  |  |
| Video-R1 |  | NeurIPS |  |  |
| time-R1 |  | NeurIPS |  |  |
| Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding | [https://arxiv.org/pdf/2505.12605](https://arxiv.org/pdf/2505.12605) |  |  |  |
|  |  |  |  |  |
|  |  |  |  |  |
| ***Visual Token Compression for Video Understanding*** |  |  |  |  |
| InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding | [https://arxiv.org/pdf/2506.15745](https://arxiv.org/pdf/2506.15745) | NeurIPS | Addressing the issue that when multimodal large language models (MLLMs) process streaming video, the KV cache grows linearly over time, exceeding the memory limitations of edge devices (such as smartphones and AR glasses). | First, removing temporally redundant tokens using the Temporal axis Redundancy (TaR) metric; second, retaining semantically important tokens through Value Norm (VaN) ranking. |
| **FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding** | [https://arxiv.org/pdf/2504.20384](https://arxiv.org/pdf/2504.20384) |  |  |  |
| Quicksviewer: An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes | [https://arxiv.org/pdf/2504.15270](https://arxiv.org/pdf/2504.15270) |  |  |  |
| Video-XL: Extra-Long Vision Language Model for Hour-Scale Video |  |  |  |  |
| Understanding | [https://arxiv.org/pdf/2409.14485](https://arxiv.org/pdf/2409.14485) |  |  |  |
| Slow-Fast Architecture for Video Multi-Modal Large Language Models | [https://arxiv.org/pdf/2504.01328](https://arxiv.org/pdf/2504.01328) |  |  |  |
| One Token per Highly Selective Frame: Towards Extreme Compression for Long Video Understanding |  - | NeurIPS |  |  |
|  |  |  |  |  |
| ***Agentic Video understanding / Video Reasoning*** |  |  |  |  |
| VideoDeepResearch: Long Video Understanding With Agentic Tool Using | [https://arxiv.org/pdf/2506.10821](https://arxiv.org/pdf/2506.10821) |  |  |  |
| Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding | [https://arxiv.org/pdf/2505.18079](https://arxiv.org/pdf/2505.18079) | NeurIPS |  |  |
| Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning | [https://arxiv.org/pdf/2506.13654](https://arxiv.org/pdf/2506.13654) |  |  |  |
| When Thinking Drifts: Evidential Grounding for Robust Video Reasoning | [https://arxiv.org/pdf/2510.06077v1](https://arxiv.org/pdf/2510.06077v1) | NeurIPS | The paper points out that the Chain-of-Thought (CoT) mechanism, which is effective in text tasks, often leads to a "visual thought drift" phenomenon in video reasoning—where reasoning drifts away from video evidence and amplifies internal biases or language priors—due to generating lengthy and misleading internal monologues and hallucinating visual details. To address this issue, the study proposes a Visual Evidence Reward (VER) reinforcement learning framework, which rewards grounded reasoning by using an auxiliary LLM to assess the alignment between reasoning trajectories and visual evidence. The effectiveness of the Video-VER model is validated across 10 video understanding benchmarks. |  |
| MR. Video: “MapReduce” is the Principle for Long Video Understanding | [https://arxiv.org/pdf/2504.16082](https://arxiv.org/pdf/2504.16082) | NeurIPS |  |  |
|  |  |  |  |  |
| ***Video Benchmark and Dataset*** |  |  |  |  |
| Towards Video Thinking Test: A Holistic Benchmark for Advanced Video |  |  |  |  |
| Reasoning and Understanding | [https://arxiv.org/pdf/2507.15028](https://arxiv.org/pdf/2507.15028) |  |  |  |
| EgoExoLearn_A_Dataset_for_Bridging_Asynchronous_Ego-_and_Exo-centric_View |  |  |  |  |
| AV-Reasoner: Improving and Benchmarking |  |  |  |  |
| Clue-Grounded Audio-Visual Counting for MLLMs | [https://arxiv.org/pdf/2506.05328](https://arxiv.org/pdf/2506.05328) |  |  |  |
| Breaking Down Video LLM Benchmarks: Knowledge, |  |  |  |  |
| Spatial Perception, or True Temporal Understanding? | [https://arxiv.org/pdf/2505.14321](https://arxiv.org/pdf/2505.14321) |  |  |  |
| Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events | [https://arxiv.org/pdf/2412.05725](https://arxiv.org/pdf/2412.05725) |  |  |  |
|  |  |  |  |  |
| ***Temporal Grounding / Perception /
Enhancement*** |  |  |  |  |
| TempSamp-R1 | [https://arxiv.org/pdf/2509.18056](https://arxiv.org/pdf/2509.18056) | NeurIPS |  | GRPO+data |
| Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement | [https://arxiv.org/abs/2510.08138](https://arxiv.org/abs/2510.08138) |  |  |  |
|  |  |  |  |  |
| ***Other Tasks*** |  |  |  |  |
| MODELING FINE-GRAINED HAND-OBJECT |  |  |  |  |
| DYNAMICS FOR EGOCENTRIC VIDEO |  |  |  |  |
| REPRESENTATION LEARNING | [https://arxiv.org/pdf/2503.00986](https://arxiv.org/pdf/2503.00986) |  |  |  |
|  |  |  |  |  |
